{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kids/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as k\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial importing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get working directory and\n",
    "WORKING_DIR = \"./data/\"\n",
    "# Location of labels\n",
    "LABELS = WORKING_DIR + \"labels.csv\"\n",
    "# Example of the submission text\n",
    "TEST = WORKING_DIR + \"sample_submission.csv\"\n",
    "\n",
    "# Location of train and test folders\n",
    "TRAIN_FOLDER = WORKING_DIR + \"/train/\"\n",
    "TEST_FOLDER = WORKING_DIR + \"/test/\"\n",
    "\n",
    "# Read in the labels and the test data\n",
    "df_train = pd.read_csv(LABELS)\n",
    "df_test = pd.read_csv(TEST)\n",
    "\n",
    "targets_series = pd.Series(df_train['breed'])\n",
    "one_hot = pd.get_dummies(targets_series, sparse=True)\n",
    "one_hot_labels = np.asarray(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:51<00:00, 198.41it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "im_size = 224 # Max size is 299\n",
    "num_output =120\n",
    "x_train_0 = []\n",
    "y_train_0 = []\n",
    "\n",
    "\n",
    "\n",
    "for f, breed in tqdm(df_train.values):\n",
    "    img = cv2.imread('{}{}.jpg'.format(TRAIN_FOLDER,f))\n",
    "    label = one_hot_labels[i]\n",
    "    x_train_0.append(cv2.resize(img, (im_size, im_size)))\n",
    "    y_train_0.append(label)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [00:52<00:00, 197.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# Resizing and retraining test dat\n",
    "x_test_0 = []\n",
    "for f in tqdm(df_test['id'].values):\n",
    "    img = cv2.imread('{}{}.jpg'.format(TEST_FOLDER,f))\n",
    "    x_test_0.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 224, 224, 3)\n",
      "(10222, 120)\n",
      "(10357, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#  Converting to 32 and from 0 to 1\n",
    "\n",
    "y_train_raw = np.array(y_train_0, np.uint8)\n",
    "x_train_raw = np.array(x_train_0, np.float32) / 255.\n",
    "x_test  = np.array(x_test_0, np.float32) / 255.\n",
    "\n",
    "# Check shape\n",
    "print(x_train_raw.shape)\n",
    "print(y_train_raw.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((8177, 224, 224, 3), (8177, 120), (2045, 224, 224, 3), (2045, 120))\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_raw, y_train_raw, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement Batch Normalization\n",
    "\n",
    "k.set_image_dim_ordering('tf')\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "# Number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is if I would like to define my own CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_cnn_model():\n",
    "    # YOUR TURN\n",
    "    # Build a model with 4 convolutional layers\n",
    "    # choose your own hyperparameters for conv layers\n",
    "    # choose to include maxpool if you like\n",
    "    # choose to include dropout if you like\n",
    "    # create model\n",
    "    inner_model = Sequential()\n",
    "\n",
    "    # Add 32 filters\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same',\n",
    "                           input_shape=(im_size, im_size, 3)))\n",
    "    inner_model.add(Activation('relu'))\n",
    "\n",
    "    # Conv2D 32 3 x3\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # 2x2 pooling\n",
    "    inner_model.add(MaxPooling2D((2, 2)))\n",
    "    # Conv2D 64  3x3\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # Conv 3D 8x8\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # 2x2 pooling\n",
    "    inner_model.add(MaxPooling2D((2, 2)))\n",
    "    # Flatten\n",
    "    inner_model.add(Flatten())\n",
    "\n",
    "    # . Density layer\n",
    "    inner_model.add(Dense(512, activation='relu'))\n",
    "    # . Density layer\n",
    "    inner_model.add(Dense(512, activation='relu'))\n",
    "    #     # output layer\n",
    "    inner_model.add(Dense(num_output, activation='softmax'))\n",
    "    inner_model.summary()\n",
    "    # Compile model using the same options\n",
    "    inner_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return inner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_model():\n",
    "    # YOUR TURN\n",
    "    # Create a model with 4 convolutional layers (2 repeating VGG stype units) and 2 dense layers before the output\n",
    "    # Use Batch Normalization for every conv and dense layers\n",
    "    # Use dropout layers if you like\n",
    "    # Use Adam optimizer\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, input_shape=(im_size, im_size, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(16, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_output))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 64)      36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               102760960 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               61560     \n",
      "=================================================================\n",
      "Total params: 103,197,752\n",
      "Trainable params: 103,197,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build the model\n",
    "\n",
    "# trained_model = \"multilayer\"  # multilayer or bn_model\n",
    "trained_model = \"multilayer\"\n",
    "if trained_model == \"multilayer\":\n",
    "    model = multilayer_cnn_model()\n",
    "elif trained_model == \"bn_model\":\n",
    "    model = bn_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/20\n",
      " - 2312s - loss: 5.0958 - acc: 0.0088 - val_loss: 4.7856 - val_acc: 0.0088\n",
      "Epoch 2/20\n",
      " - 2197s - loss: 4.7822 - acc: 0.0113 - val_loss: 4.7822 - val_acc: 0.0088\n",
      "Epoch 3/20\n",
      " - 2170s - loss: 4.7826 - acc: 0.0120 - val_loss: 4.7808 - val_acc: 0.0108\n",
      "Epoch 4/20\n",
      " - 2161s - loss: 4.7736 - acc: 0.0117 - val_loss: 4.7767 - val_acc: 0.0093\n",
      "Epoch 5/20\n",
      " - 2163s - loss: 4.7114 - acc: 0.0215 - val_loss: 4.6834 - val_acc: 0.0284\n",
      "Epoch 6/20\n",
      " - 2167s - loss: 4.1926 - acc: 0.1028 - val_loss: 4.9246 - val_acc: 0.0303\n",
      "Epoch 7/20\n",
      " - 2186s - loss: 2.3993 - acc: 0.4388 - val_loss: 6.7260 - val_acc: 0.0328\n",
      "Epoch 8/20\n",
      " - 2205s - loss: 0.6860 - acc: 0.8395 - val_loss: 9.6178 - val_acc: 0.0337\n",
      "Epoch 9/20\n",
      " - 2209s - loss: 0.2592 - acc: 0.9450 - val_loss: 10.9439 - val_acc: 0.0308\n",
      "Epoch 10/20\n",
      " - 2212s - loss: 0.1562 - acc: 0.9688 - val_loss: 11.3113 - val_acc: 0.0254\n",
      "Epoch 11/20\n",
      " - 2215s - loss: 0.0965 - acc: 0.9810 - val_loss: 11.9167 - val_acc: 0.0313\n",
      "Epoch 12/20\n",
      " - 2214s - loss: 0.0720 - acc: 0.9870 - val_loss: 11.9660 - val_acc: 0.0289\n",
      "Epoch 13/20\n",
      " - 2219s - loss: 0.0491 - acc: 0.9910 - val_loss: 12.7218 - val_acc: 0.0337\n",
      "Epoch 14/20\n",
      " - 2223s - loss: 0.0576 - acc: 0.9906 - val_loss: 11.8335 - val_acc: 0.0259\n",
      "Epoch 15/20\n",
      " - 2220s - loss: 0.0540 - acc: 0.9897 - val_loss: 12.5614 - val_acc: 0.0284\n",
      "Epoch 16/20\n",
      " - 2225s - loss: 0.0349 - acc: 0.9946 - val_loss: 12.6499 - val_acc: 0.0308\n",
      "Epoch 17/20\n",
      " - 2244s - loss: 0.0297 - acc: 0.9958 - val_loss: 12.8836 - val_acc: 0.0323\n",
      "Epoch 18/20\n",
      " - 2247s - loss: 0.0262 - acc: 0.9969 - val_loss: 12.6937 - val_acc: 0.0269\n",
      "Epoch 19/20\n",
      " - 2217s - loss: 0.0274 - acc: 0.9962 - val_loss: 12.3677 - val_acc: 0.0279\n",
      "Epoch 20/20\n",
      " - 2223s - loss: 0.0255 - acc: 0.9966 - val_loss: 12.4502 - val_acc: 0.0274\n",
      "Baseline Error: 97.26%\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=200,\n",
    "                    verbose=2)  # Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100 - scores[1] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "# serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"{}/{}_model.json\".format(WORKING_DIR,trained_model), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"{}/{}_model.h5\".format(WORKING_DIR,trained_model))\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - 937s 90ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = model.predict(x_test, verbose=1)\n",
    "\n",
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "# Saving results\n",
    "sub.to_csv(\"{}/results_{}.csv\".format(WORKING_DIR, trained_model), mode='w', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for running against previous defined models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the user can select from a few different models that have allready been used.  More information at the website https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model = \"vgg16\"  # inception, vgg16, resnet50, mobilenet\n",
    "# trained_model = \"inception\"\n",
    "# trained_model = \"resnet50\"\n",
    "trained_model = \"mobilenet\"\n",
    "\n",
    "# Idea from https://keras.io/applications/\n",
    "if trained_model == \"inception\":\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"vgg16\":\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"resnet50\":\n",
    "    base_model =  ResNet50(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"mobilenet\":\n",
    "    base_model = MobileNet(input_shape=(im_size, im_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "else:\n",
    "    sys.exit(\" Could not find the train model to start with\")\n",
    "# add a global spatial average pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we start with the base model\n",
    "x = base_model.output\n",
    "# Then we add pooling 2d and a FCC layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer with the final guess\n",
    "predictions = Dense(num_output, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code idea came from https://gist.github.com/Hironsan/e041d6606164bc14c50aa56b989c5fc0\n",
    "# Function came from https://gist.github.com/Hironsan/e041d6606164bc14c50aa56b989c5fc0\n",
    "def batch_iter(data, labels, batch_size_def, shuffle=False):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size_def) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size_def\n",
    "                end_index = min((batch_num + 1) * batch_size_def, data_size)\n",
    "                x_value, y_value = shuffled_data[start_index: end_index], shuffled_labels[start_index: end_index]\n",
    "                yield x_value, y_value\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "256/256 [==============================] - 660s 3s/step - loss: 0.1134 - acc: 0.9775 - val_loss: 2.1735 - val_acc: 0.5164\n",
      "Epoch 2/30\n",
      "256/256 [==============================] - 675s 3s/step - loss: 0.0825 - acc: 0.9857 - val_loss: 2.3492 - val_acc: 0.5032\n",
      "Epoch 3/30\n",
      "256/256 [==============================] - 677s 3s/step - loss: 0.0722 - acc: 0.9841 - val_loss: 2.1394 - val_acc: 0.5340\n",
      "Epoch 4/30\n",
      "256/256 [==============================] - 684s 3s/step - loss: 0.0653 - acc: 0.9878 - val_loss: 2.2761 - val_acc: 0.5394\n",
      "Epoch 5/30\n",
      "256/256 [==============================] - 653s 3s/step - loss: 0.0692 - acc: 0.9835 - val_loss: 2.3547 - val_acc: 0.5350\n",
      "Epoch 6/30\n",
      "256/256 [==============================] - 635s 2s/step - loss: 0.0658 - acc: 0.9840 - val_loss: 2.7690 - val_acc: 0.5022\n",
      "Epoch 7/30\n",
      "256/256 [==============================] - 632s 2s/step - loss: 0.0608 - acc: 0.9852 - val_loss: 2.5448 - val_acc: 0.5159\n",
      "Epoch 8/30\n",
      "256/256 [==============================] - 630s 2s/step - loss: 0.0513 - acc: 0.9878 - val_loss: 2.5629 - val_acc: 0.5213\n",
      "Epoch 9/30\n",
      "256/256 [==============================] - 631s 2s/step - loss: 0.0530 - acc: 0.9862 - val_loss: 2.8648 - val_acc: 0.5061\n",
      "Epoch 10/30\n",
      "256/256 [==============================] - 631s 2s/step - loss: 0.1016 - acc: 0.9680 - val_loss: 3.0442 - val_acc: 0.4846\n",
      "Epoch 11/30\n",
      "256/256 [==============================] - 632s 2s/step - loss: 0.1001 - acc: 0.9699 - val_loss: 2.8288 - val_acc: 0.5271\n",
      "Epoch 12/30\n",
      "256/256 [==============================] - 631s 2s/step - loss: 0.1023 - acc: 0.9658 - val_loss: 2.8290 - val_acc: 0.5301\n",
      "Epoch 13/30\n",
      "256/256 [==============================] - 634s 2s/step - loss: 0.0583 - acc: 0.9814 - val_loss: 2.8391 - val_acc: 0.5433\n",
      "Epoch 14/30\n",
      "256/256 [==============================] - 637s 2s/step - loss: 0.0507 - acc: 0.9846 - val_loss: 2.9069 - val_acc: 0.5315\n",
      "Epoch 15/30\n",
      "256/256 [==============================] - 641s 3s/step - loss: 0.0391 - acc: 0.9878 - val_loss: 2.9849 - val_acc: 0.5281\n",
      "Epoch 16/30\n",
      "256/256 [==============================] - 636s 2s/step - loss: 0.0533 - acc: 0.9843 - val_loss: 2.8728 - val_acc: 0.5472\n",
      "Epoch 17/30\n",
      "256/256 [==============================] - 638s 2s/step - loss: 0.0425 - acc: 0.9889 - val_loss: 2.7958 - val_acc: 0.5438\n",
      "Epoch 18/30\n",
      "256/256 [==============================] - 639s 2s/step - loss: 0.0243 - acc: 0.9933 - val_loss: 2.9990 - val_acc: 0.5408\n",
      "Epoch 19/30\n",
      "256/256 [==============================] - 636s 2s/step - loss: 0.0763 - acc: 0.9763 - val_loss: 3.0296 - val_acc: 0.5257\n",
      "Epoch 20/30\n",
      "256/256 [==============================] - 638s 2s/step - loss: 0.1023 - acc: 0.9656 - val_loss: 3.1870 - val_acc: 0.5139\n",
      "Epoch 21/30\n",
      "256/256 [==============================] - 637s 2s/step - loss: 0.0411 - acc: 0.9877 - val_loss: 3.2548 - val_acc: 0.5252\n",
      "Epoch 22/30\n",
      "256/256 [==============================] - 637s 2s/step - loss: 0.0445 - acc: 0.9866 - val_loss: 3.3429 - val_acc: 0.5164\n",
      "Epoch 23/30\n",
      "256/256 [==============================] - 639s 2s/step - loss: 0.0636 - acc: 0.9778 - val_loss: 3.5593 - val_acc: 0.5076\n",
      "Epoch 24/30\n",
      "256/256 [==============================] - 636s 2s/step - loss: 0.0602 - acc: 0.9792 - val_loss: 3.4429 - val_acc: 0.4993\n",
      "Epoch 25/30\n",
      "256/256 [==============================] - 638s 2s/step - loss: 0.0383 - acc: 0.9878 - val_loss: 3.2318 - val_acc: 0.5311\n",
      "Epoch 26/30\n",
      "256/256 [==============================] - 637s 2s/step - loss: 0.0141 - acc: 0.9960 - val_loss: 3.5751 - val_acc: 0.5203\n",
      "Epoch 27/30\n",
      "256/256 [==============================] - 637s 2s/step - loss: 0.0442 - acc: 0.9868 - val_loss: 3.6478 - val_acc: 0.5169\n",
      "Epoch 28/30\n",
      "256/256 [==============================] - 638s 2s/step - loss: 0.0613 - acc: 0.9789 - val_loss: 3.6548 - val_acc: 0.5134\n",
      "Epoch 29/30\n",
      "256/256 [==============================] - 638s 2s/step - loss: 0.0640 - acc: 0.9792 - val_loss: 3.4614 - val_acc: 0.5071\n",
      "Epoch 30/30\n",
      "256/256 [==============================] - 641s 3s/step - loss: 0.0373 - acc: 0.9883 - val_loss: 3.5737 - val_acc: 0.5203\n",
      "(19249.035948991776, ' seconds')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#. Here we only need a few epochs because we started with a trained model\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "#. Select different train and valid steps for each batch to switch things up\n",
    "train_steps, train_batches = batch_iter(X_train, y_train, batch_size)\n",
    "valid_steps, valid_batches = batch_iter(X_test, y_test, batch_size)\n",
    "# Want to figure out the time of the different methods\n",
    "t0=time.time()\n",
    "model.fit_generator(train_batches, train_steps, epochs=num_epochs, validation_data=valid_batches,\n",
    "                    validation_steps=valid_steps)\n",
    "t1=time.time()\n",
    "print(t1-t0,\" seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different methods used here were:\n",
    "* Vgg16 :  Got score of 1.56173 on kaggle\n",
    "* Inception: Got a score of  \n",
    "* resnet50:  Got a score of 5.8588 on kaggle.  Seemed to be overfitting the problem\n",
    "* mobilenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10357/10357 [==============================] - 699s 67ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "# Saving results\n",
    "sub.to_csv(\"{}/results_{}.csv\".format(WORKING_DIR, trained_model), mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to https://www.kaggle.com/c/dog-breed-identification/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
