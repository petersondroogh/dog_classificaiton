{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as k\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial importing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get working directory and\n",
    "WORKING_DIR = \"./data/\"\n",
    "# Location of labels\n",
    "LABELS = WORKING_DIR + \"labels.csv\"\n",
    "# Example of the submission text\n",
    "TEST = WORKING_DIR + \"sample_submission.csv\"\n",
    "\n",
    "# Location of train and test folders\n",
    "TRAIN_FOLDER = WORKING_DIR + \"/train/\"\n",
    "TEST_FOLDER = WORKING_DIR + \"/test/\"\n",
    "\n",
    "# Read in the labels and the test data\n",
    "df_train = pd.read_csv(LABELS)\n",
    "df_test = pd.read_csv(TEST)\n",
    "\n",
    "targets_series = pd.Series(df_train['breed'])\n",
    "one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "one_hot_labels = np.asarray(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output =120\n",
    "im_size = 224 # Max size is 299\n",
    "x_train_0 = []\n",
    "y_train_0 = []\n",
    "x_pred_0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:50<00:00, 201.68it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "for f, breed in tqdm(df_train.values):\n",
    "    img = cv2.imread('{}{}.jpg'.format(TRAIN_FOLDER,f))\n",
    "    label = one_hot_labels[i]\n",
    "    x_train_0.append(cv2.resize(img, (im_size, im_size)))\n",
    "    y_train_0.append(label)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [00:49<00:00, 211.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Resizing and retraining test dat\n",
    "for f in tqdm(df_test['id'].values):\n",
    "    img = cv2.imread('{}{}.jpg'.format(TEST_FOLDER,f))\n",
    "    x_pred_0.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Converting to 32 and from 0 to 1\n",
    "\n",
    "y_train_raw = np.array(y_train_0, np.uint8)\n",
    "x_train_raw = np.array(x_train_0, np.float32) / 255\n",
    "x_pred_raw = np.array(x_pred_0, np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 224, 224, 3)\n",
      "(10222, 120)\n",
      "(10357, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check shape\n",
    "print(x_train_raw.shape)\n",
    "print(y_train_raw.shape)\n",
    "print(x_pred_raw.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((8177, 224, 224, 3), (8177, 120), (2045, 224, 224, 3), (2045, 120))\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_raw, y_train_raw, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is if I would like to define my own CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_cnn_model():\n",
    "    # YOUR TURN\n",
    "    # Build a model with 4 convolutional layers\n",
    "    # choose your own hyperparameters for conv layers\n",
    "    # choose to include maxpool if you like\n",
    "    # choose to include dropout if you like\n",
    "    # create model\n",
    "    inner_model = Sequential()\n",
    "\n",
    "    # Add 32 filters\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same',\n",
    "                           input_shape=(im_size, im_size, 3)))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    inner_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Conv2D 32 3 x3\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # 2x2 pooling\n",
    "    inner_model.add(MaxPooling2D((2, 2)))\n",
    "    # Conv2D 64  3x3\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # Conv 3D 8x8\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # 2x2 pooling\n",
    "    inner_model.add(MaxPooling2D((2, 2)))\n",
    "    # Flatten\n",
    "    inner_model.add(Flatten())\n",
    "\n",
    "    # . Density layer\n",
    "    inner_model.add(Dense(512, activation='relu'))\n",
    "    # . Density layer\n",
    "    inner_model.add(Dense(512, activation='relu'))\n",
    "    #     # output layer\n",
    "    inner_model.add(Dense(num_output, activation='softmax'))\n",
    "    inner_model.summary()\n",
    "    # Compile model using the same options\n",
    "    inner_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return inner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_model():\n",
    "    # YOUR TURN\n",
    "    # Create a model with 4 convolutional layers (2 repeating VGG stype units) and 2 dense layers before the output\n",
    "    # Use Batch Normalization for every conv and dense layers\n",
    "    # Use dropout layers if you like\n",
    "    # Use Adam optimizer\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, input_shape=(im_size, im_size, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_output))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea from https://github.com/mvrhine/Kaggle---Dog-breed-classification/blob/master/Dog%20Breed%20Classification%20-%20Kaggle.ipynb\n",
    "def model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), input_shape=(im_size,im_size,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(216, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_output, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build the model\n",
    "\n",
    "# trained_model = \"multilayer\"  # multilayer or bn_model model_1\n",
    "trained_model = \"multilayer\"\n",
    "trained_model = \"model_1\"\n",
    "if trained_model == \"multilayer\":\n",
    "    model = multilayer_cnn_model()\n",
    "elif trained_model == \"bn_model\":\n",
    "    model = bn_model()\n",
    "elif trained_model == \"model_1\":\n",
    "    model = model_1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/50\n",
      "8177/8177 [==============================] - 447s 55ms/step - loss: 4.7881 - acc: 0.0088 - val_loss: 4.7853 - val_acc: 0.0093\n",
      "Epoch 2/50\n",
      "8177/8177 [==============================] - 389s 48ms/step - loss: 4.7773 - acc: 0.0139 - val_loss: 4.7494 - val_acc: 0.0200\n",
      "Epoch 3/50\n",
      "8177/8177 [==============================] - 324s 40ms/step - loss: 4.7001 - acc: 0.0183 - val_loss: 4.6556 - val_acc: 0.0254\n",
      "Epoch 4/50\n",
      "3600/8177 [============>.................] - ETA: 2:55 - loss: 4.6339 - acc: 0.0197"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=200,\n",
    "                    verbose=1)  # Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100 - scores[1] * 100))\n",
    "\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "scores1 = model.evaluate(X_train, Y_train)\n",
    "print (\"Test score - {}\".format(scores[0]))\n",
    "print (\"Test accuracy - {}\".format(scores[1]))\n",
    "print (\"Train score - {}\".format(scores1[0]))\n",
    "print (\"Train accuracy - {}\".format(scores1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "# serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"{}/{}_model.json\".format(WORKING_DIR,trained_model), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"{}/{}_model.h5\".format(WORKING_DIR,trained_model))\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = model.predict(x_pred, verbose=1)\n",
    "\n",
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "# Saving results\n",
    "sub.to_csv(\"{}/results_{}.csv\".format(WORKING_DIR, trained_model), mode='w', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for running against previous defined models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the user can select from a few different models that have allready been used.  More information at the website https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model = \"vgg16\"  # inception, vgg16, resnet50, mobilenet\n",
    "# trained_model = \"inception\"\n",
    "# trained_model = \"resnet50\"\n",
    "trained_model = \"mobilenet\"\n",
    "\n",
    "# Idea from https://keras.io/applications/\n",
    "if trained_model == \"inception\":\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"vgg16\":\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"resnet50\":\n",
    "    base_model =  ResNet50(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"mobilenet\":\n",
    "    base_model = MobileNet(input_shape=(im_size, im_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "else:\n",
    "    sys.exit(\" Could not find the train model to start with\")\n",
    "# add a global spatial average pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we start with the base model\n",
    "x = base_model.output\n",
    "# Then we add pooling 2d and a FCC layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer with the final guess\n",
    "predictions = Dense(num_output, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code idea came from https://gist.github.com/Hironsan/e041d6606164bc14c50aa56b989c5fc0\n",
    "# Function came from https://gist.github.com/Hironsan/e041d6606164bc14c50aa56b989c5fc0\n",
    "def batch_iter(data, labels, batch_size_def, shuffle=False):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size_def) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size_def\n",
    "                end_index = min((batch_num + 1) * batch_size_def, data_size)\n",
    "                x_value, y_value = shuffled_data[start_index: end_index], shuffled_labels[start_index: end_index]\n",
    "                yield x_value, y_value\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#. Here we only need a few epochs because we started with a trained model\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "#. Select different train and valid steps for each batch to switch things up\n",
    "train_steps, train_batches = batch_iter(X_train, y_train, batch_size)\n",
    "valid_steps, valid_batches = batch_iter(X_test, y_test, batch_size)\n",
    "# Want to figure out the time of the different methods\n",
    "t0=time.time()\n",
    "model.fit_generator(train_batches, train_steps, epochs=num_epochs, validation_data=valid_batches,\n",
    "                    validation_steps=valid_steps)\n",
    "t1=time.time()\n",
    "print(t1-t0,\" seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different methods used here were:\n",
    "* Vgg16 :  Got score of 1.56173 on kaggle\n",
    "* Inception: Got a score of  \n",
    "* resnet50:  Got a score of 5.8588 on kaggle.  Seemed to be overfitting the problem\n",
    "* mobilenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_pred_raw, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "# Saving results\n",
    "sub.to_csv(\"{}/results_{}.csv\".format(WORKING_DIR, trained_model), mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to https://www.kaggle.com/c/dog-breed-identification/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
