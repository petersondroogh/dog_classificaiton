{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as k\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intial importing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get working directory and\n",
    "WORKING_DIR = \"./data/\"\n",
    "# Location of labels\n",
    "LABELS = WORKING_DIR + \"labels.csv\"\n",
    "# Example of the submission text\n",
    "TEST = WORKING_DIR + \"sample_submission.csv\"\n",
    "\n",
    "# Location of train and test folders\n",
    "TRAIN_FOLDER = WORKING_DIR + \"/train/\"\n",
    "TEST_FOLDER = WORKING_DIR + \"/test/\"\n",
    "\n",
    "# Read in the labels and the test data\n",
    "df_train = pd.read_csv(LABELS)\n",
    "df_test = pd.read_csv(TEST)\n",
    "\n",
    "targets_series = pd.Series(df_train['breed'])\n",
    "one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "one_hot_labels = np.asarray(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_output =120\n",
    "im_size = 224 # Max size is 299\n",
    "x_train_0 = []\n",
    "y_train_0 = []\n",
    "x_pred_0 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:50<00:00, 201.68it/s]\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "for f, breed in tqdm(df_train.values):\n",
    "    img = cv2.imread('{}{}.jpg'.format(TRAIN_FOLDER,f))\n",
    "    label = one_hot_labels[i]\n",
    "    x_train_0.append(cv2.resize(img, (im_size, im_size)))\n",
    "    y_train_0.append(label)\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10357/10357 [00:49<00:00, 211.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# Resizing and retraining test dat\n",
    "for f in tqdm(df_test['id'].values):\n",
    "    img = cv2.imread('{}{}.jpg'.format(TEST_FOLDER,f))\n",
    "    x_pred_0.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Converting to 32 and from 0 to 1\n",
    "\n",
    "y_train_raw = np.array(y_train_0, np.uint8)\n",
    "x_train_raw = np.array(x_train_0, np.float32) / 255\n",
    "x_pred_raw = np.array(x_pred_0, np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 224, 224, 3)\n",
      "(10222, 120)\n",
      "(10357, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check shape\n",
    "print(x_train_raw.shape)\n",
    "print(y_train_raw.shape)\n",
    "print(x_pred_raw.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((8177, 224, 224, 3), (8177, 120), (2045, 224, 224, 3), (2045, 120))\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_raw, y_train_raw, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is if I would like to define my own CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_cnn_model():\n",
    "    # YOUR TURN\n",
    "    # Build a model with 4 convolutional layers\n",
    "    # choose your own hyperparameters for conv layers\n",
    "    # choose to include maxpool if you like\n",
    "    # choose to include dropout if you like\n",
    "    # create model\n",
    "    inner_model = Sequential()\n",
    "\n",
    "    # Add 32 filters\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same',\n",
    "                           input_shape=(im_size, im_size, 3)))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    inner_model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Conv2D 32 3 x3\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # 2x2 pooling\n",
    "    inner_model.add(MaxPooling2D((2, 2)))\n",
    "    # Conv2D 64  3x3\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # Conv 3D 8x8\n",
    "    inner_model.add(Conv2D(64, kernel_size=3, padding='same'))\n",
    "    inner_model.add(Activation('relu'))\n",
    "    # 2x2 pooling\n",
    "    inner_model.add(MaxPooling2D((2, 2)))\n",
    "    # Flatten\n",
    "    inner_model.add(Flatten())\n",
    "\n",
    "    # . Density layer\n",
    "    inner_model.add(Dense(512, activation='relu'))\n",
    "    # . Density layer\n",
    "    inner_model.add(Dense(512, activation='relu'))\n",
    "    #     # output layer\n",
    "    inner_model.add(Dense(num_output, activation='softmax'))\n",
    "    inner_model.summary()\n",
    "    # Compile model using the same options\n",
    "    inner_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return inner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_model():\n",
    "    # YOUR TURN\n",
    "    # Create a model with 4 convolutional layers (2 repeating VGG stype units) and 2 dense layers before the output\n",
    "    # Use Batch Normalization for every conv and dense layers\n",
    "    # Use dropout layers if you like\n",
    "    # Use Adam optimizer\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, 3, input_shape=(im_size, im_size, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(16, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(32, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, 3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_output))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea from https://github.com/mvrhine/Kaggle---Dog-breed-classification/blob/master/Dog%20Breed%20Classification%20-%20Kaggle.ipynb\n",
    "def model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), input_shape=(im_size,im_size,3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(216, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_output, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build the model\n",
    "\n",
    "# trained_model = \"multilayer\"  # multilayer or bn_model model_1\n",
    "trained_model = \"multilayer\"\n",
    "trained_model = \"model_1\"\n",
    "if trained_model == \"multilayer\":\n",
    "    model = multilayer_cnn_model()\n",
    "elif trained_model == \"bn_model\":\n",
    "    model = bn_model()\n",
    "elif trained_model == \"model_1\":\n",
    "    model = model_1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8177 samples, validate on 2045 samples\n",
      "Epoch 1/50\n",
      "8177/8177 [==============================] - 447s 55ms/step - loss: 4.7881 - acc: 0.0088 - val_loss: 4.7853 - val_acc: 0.0093\n",
      "Epoch 2/50\n",
      "8177/8177 [==============================] - 389s 48ms/step - loss: 4.7773 - acc: 0.0139 - val_loss: 4.7494 - val_acc: 0.0200\n",
      "Epoch 3/50\n",
      "8177/8177 [==============================] - 324s 40ms/step - loss: 4.7001 - acc: 0.0183 - val_loss: 4.6556 - val_acc: 0.0254\n",
      "Epoch 4/50\n",
      "8177/8177 [==============================] - 356s 44ms/step - loss: 4.6148 - acc: 0.0198 - val_loss: 4.5811 - val_acc: 0.0289\n",
      "Epoch 5/50\n",
      "8177/8177 [==============================] - 348s 43ms/step - loss: 4.5086 - acc: 0.0270 - val_loss: 4.5248 - val_acc: 0.0313\n",
      "Epoch 6/50\n",
      "8177/8177 [==============================] - 295s 36ms/step - loss: 4.3865 - acc: 0.0394 - val_loss: 4.3804 - val_acc: 0.0435\n",
      "Epoch 7/50\n",
      "8177/8177 [==============================] - 288s 35ms/step - loss: 4.2604 - acc: 0.0494 - val_loss: 4.2819 - val_acc: 0.0509\n",
      "Epoch 8/50\n",
      "8177/8177 [==============================] - 347s 42ms/step - loss: 4.1310 - acc: 0.0615 - val_loss: 4.3220 - val_acc: 0.0499\n",
      "Epoch 9/50\n",
      "8177/8177 [==============================] - 362s 44ms/step - loss: 4.0171 - acc: 0.0741 - val_loss: 4.2059 - val_acc: 0.0641\n",
      "Epoch 10/50\n",
      "8177/8177 [==============================] - 339s 41ms/step - loss: 3.9122 - acc: 0.0899 - val_loss: 4.2040 - val_acc: 0.0660\n",
      "Epoch 11/50\n",
      "8177/8177 [==============================] - 313s 38ms/step - loss: 3.7192 - acc: 0.1202 - val_loss: 4.2099 - val_acc: 0.0743\n",
      "Epoch 12/50\n",
      "8177/8177 [==============================] - 285s 35ms/step - loss: 3.4647 - acc: 0.1584 - val_loss: 4.3441 - val_acc: 0.0787\n",
      "Epoch 13/50\n",
      "8177/8177 [==============================] - 332s 41ms/step - loss: 3.1417 - acc: 0.2105 - val_loss: 4.5360 - val_acc: 0.0694\n",
      "Epoch 14/50\n",
      "8177/8177 [==============================] - 303s 37ms/step - loss: 2.7026 - acc: 0.3008 - val_loss: 4.7838 - val_acc: 0.0714\n",
      "Epoch 15/50\n",
      "8177/8177 [==============================] - 284s 35ms/step - loss: 2.2178 - acc: 0.4147 - val_loss: 5.2952 - val_acc: 0.0650\n",
      "Epoch 16/50\n",
      "8177/8177 [==============================] - 284s 35ms/step - loss: 1.6926 - acc: 0.5355 - val_loss: 6.2497 - val_acc: 0.0743\n",
      "Epoch 17/50\n",
      "8177/8177 [==============================] - 300s 37ms/step - loss: 1.2633 - acc: 0.6434 - val_loss: 6.9129 - val_acc: 0.0670\n",
      "Epoch 18/50\n",
      "8177/8177 [==============================] - 261s 32ms/step - loss: 0.9594 - acc: 0.7197 - val_loss: 7.8461 - val_acc: 0.0655\n",
      "Epoch 19/50\n",
      "8177/8177 [==============================] - 272s 33ms/step - loss: 0.7121 - acc: 0.7927 - val_loss: 8.4821 - val_acc: 0.0670\n",
      "Epoch 20/50\n",
      "8177/8177 [==============================] - 265s 32ms/step - loss: 0.5128 - acc: 0.8451 - val_loss: 9.0713 - val_acc: 0.0655\n",
      "Epoch 21/50\n",
      "8177/8177 [==============================] - 271s 33ms/step - loss: 0.3971 - acc: 0.8784 - val_loss: 9.6684 - val_acc: 0.0636\n",
      "Epoch 22/50\n",
      "8177/8177 [==============================] - 285s 35ms/step - loss: 0.3157 - acc: 0.9095 - val_loss: 9.7949 - val_acc: 0.0685\n",
      "Epoch 23/50\n",
      "8177/8177 [==============================] - 269s 33ms/step - loss: 0.2550 - acc: 0.9247 - val_loss: 10.0305 - val_acc: 0.0699\n",
      "Epoch 24/50\n",
      "8177/8177 [==============================] - 273s 33ms/step - loss: 0.2082 - acc: 0.9396 - val_loss: 10.1051 - val_acc: 0.0787\n",
      "Epoch 25/50\n",
      "8177/8177 [==============================] - 276s 34ms/step - loss: 0.1841 - acc: 0.9477 - val_loss: 10.5211 - val_acc: 0.0778\n",
      "Epoch 26/50\n",
      "8177/8177 [==============================] - 276s 34ms/step - loss: 0.2073 - acc: 0.9407 - val_loss: 10.5184 - val_acc: 0.0704\n",
      "Epoch 27/50\n",
      "8177/8177 [==============================] - 270s 33ms/step - loss: 0.1828 - acc: 0.9483 - val_loss: 10.3583 - val_acc: 0.0709\n",
      "Epoch 28/50\n",
      "8177/8177 [==============================] - 272s 33ms/step - loss: 0.1401 - acc: 0.9634 - val_loss: 10.8182 - val_acc: 0.0768\n",
      "Epoch 29/50\n",
      "8177/8177 [==============================] - 257s 31ms/step - loss: 0.1260 - acc: 0.9662 - val_loss: 10.4730 - val_acc: 0.0719\n",
      "Epoch 30/50\n",
      "8177/8177 [==============================] - 254s 31ms/step - loss: 0.0975 - acc: 0.9726 - val_loss: 10.8260 - val_acc: 0.0748\n",
      "Epoch 31/50\n",
      "8177/8177 [==============================] - 249s 30ms/step - loss: 0.0824 - acc: 0.9760 - val_loss: 11.1247 - val_acc: 0.0689\n",
      "Epoch 32/50\n",
      "8177/8177 [==============================] - 272s 33ms/step - loss: 0.0969 - acc: 0.9722 - val_loss: 11.1651 - val_acc: 0.0699\n",
      "Epoch 33/50\n",
      "8177/8177 [==============================] - 277s 34ms/step - loss: 0.0844 - acc: 0.9768 - val_loss: 11.0753 - val_acc: 0.0685\n",
      "Epoch 34/50\n",
      "8177/8177 [==============================] - 269s 33ms/step - loss: 0.0784 - acc: 0.9788 - val_loss: 11.0338 - val_acc: 0.0694\n",
      "Epoch 35/50\n",
      "8177/8177 [==============================] - 277s 34ms/step - loss: 0.0833 - acc: 0.9763 - val_loss: 11.0533 - val_acc: 0.0733\n",
      "Epoch 36/50\n",
      "8177/8177 [==============================] - 289s 35ms/step - loss: 0.0799 - acc: 0.9764 - val_loss: 11.2982 - val_acc: 0.0699\n",
      "Epoch 37/50\n",
      "8177/8177 [==============================] - 271s 33ms/step - loss: 0.0674 - acc: 0.9810 - val_loss: 11.2331 - val_acc: 0.0699\n",
      "Epoch 38/50\n",
      "8177/8177 [==============================] - 277s 34ms/step - loss: 0.0625 - acc: 0.9819 - val_loss: 11.4498 - val_acc: 0.0665\n",
      "Epoch 39/50\n",
      "8177/8177 [==============================] - 262s 32ms/step - loss: 0.0847 - acc: 0.9758 - val_loss: 11.3530 - val_acc: 0.0773\n",
      "Epoch 40/50\n",
      "8177/8177 [==============================] - 255s 31ms/step - loss: 0.0667 - acc: 0.9813 - val_loss: 11.4725 - val_acc: 0.0606\n",
      "Epoch 41/50\n",
      "8177/8177 [==============================] - 252s 31ms/step - loss: 0.0888 - acc: 0.9742 - val_loss: 11.4843 - val_acc: 0.0631\n",
      "Epoch 42/50\n",
      "8177/8177 [==============================] - 251s 31ms/step - loss: 0.0811 - acc: 0.9748 - val_loss: 11.2797 - val_acc: 0.0650\n",
      "Epoch 43/50\n",
      "8177/8177 [==============================] - 307s 38ms/step - loss: 0.0562 - acc: 0.9845 - val_loss: 11.5812 - val_acc: 0.0694\n",
      "Epoch 44/50\n",
      "8000/8177 [============================>.] - ETA: 5s - loss: 0.0524 - acc: 0.9848 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-373190617314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=200,\n\u001b[0;32m----> 2\u001b[0;31m                     verbose=1)  # Final evaluation of the model\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Baseline Error: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                             val_outs = self._test_loop(val_f, val_ins,\n\u001b[1;32m   1248\u001b[0m                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                                                        verbose=0)\n\u001b[0m\u001b[1;32m   1250\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                                 \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_test_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=64,\n",
    "                    verbose=1)  # Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100 - scores[1] * 100))\n",
    "\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "scores1 = model.evaluate(X_train, Y_train)\n",
    "print (\"Test score - {}\".format(scores[0]))\n",
    "print (\"Test accuracy - {}\".format(scores[1]))\n",
    "print (\"Train score - {}\".format(scores1[0]))\n",
    "print (\"Train accuracy - {}\".format(scores1[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "# serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"{}/{}_model.json\".format(WORKING_DIR,trained_model), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"{}/{}_model.h5\".format(WORKING_DIR,trained_model))\n",
    "    print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = model.predict(x_pred, verbose=1)\n",
    "\n",
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "# Saving results\n",
    "sub.to_csv(\"{}/results_{}.csv\".format(WORKING_DIR, trained_model), mode='w', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for running against previous defined models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the user can select from a few different models that have allready been used.  More information at the website https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 1s 0us/step\n",
      "17235968/17225924 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# trained_model = \"vgg16\"  # inception, vgg16, resnet50, mobilenet\n",
    "# trained_model = \"inception\"\n",
    "# trained_model = \"resnet50\"\n",
    "trained_model = \"mobilenet\"\n",
    "\n",
    "# Idea from https://keras.io/applications/\n",
    "if trained_model == \"inception\":\n",
    "    # create the base pre-trained model\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"vgg16\":\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"resnet50\":\n",
    "    base_model =  ResNet50(weights='imagenet', include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "elif trained_model == \"mobilenet\":\n",
    "    base_model = MobileNet(input_shape=(im_size, im_size, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "else:\n",
    "    sys.exit(\" Could not find the train model to start with\")\n",
    "# add a global spatial average pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we start with the base model\n",
    "x = base_model.output\n",
    "# Then we add pooling 2d and a FCC layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer with the final guess\n",
    "predictions = Dense(num_output, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code idea came from https://gist.github.com/Hironsan/e041d6606164bc14c50aa56b989c5fc0\n",
    "# Function came from https://gist.github.com/Hironsan/e041d6606164bc14c50aa56b989c5fc0\n",
    "def batch_iter(data, labels, batch_size_def, shuffle=False):\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size_def) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data)\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = data[shuffle_indices]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size_def\n",
    "                end_index = min((batch_num + 1) * batch_size_def, data_size)\n",
    "                x_value, y_value = shuffled_data[start_index: end_index], shuffled_labels[start_index: end_index]\n",
    "                yield x_value, y_value\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "256/256 [==============================] - 721s 3s/step - loss: 2.5151 - acc: 0.3560 - val_loss: 1.8395 - val_acc: 0.4567\n",
      "Epoch 2/3\n",
      "256/256 [==============================] - 716s 3s/step - loss: 1.2524 - acc: 0.6326 - val_loss: 1.7717 - val_acc: 0.4831\n",
      "Epoch 3/3\n",
      "256/256 [==============================] - 734s 3s/step - loss: 0.7925 - acc: 0.7766 - val_loss: 1.7806 - val_acc: 0.4870\n",
      "(2171.738512992859, ' seconds')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#. Here we only need a few epochs because we started with a trained model\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "\n",
    "#. Select different train and valid steps for each batch to switch things up\n",
    "train_steps, train_batches = batch_iter(X_train, y_train, batch_size)\n",
    "valid_steps, valid_batches = batch_iter(X_test, y_test, batch_size)\n",
    "# Want to figure out the time of the different methods\n",
    "t0=time.time()\n",
    "model.fit_generator(train_batches, train_steps, epochs=num_epochs, validation_data=valid_batches,\n",
    "                    validation_steps=valid_steps)\n",
    "t1=time.time()\n",
    "print(t1-t0,\" seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different methods used here were:\n",
    "* Vgg16 :  Got score of 1.56173 on kaggle\n",
    "* Inception: Got a score of  \n",
    "* resnet50:  Got a score of 5.8588 on kaggle.  Seemed to be overfitting the problem\n",
    "* mobilenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3904/10357 [==========>...................] - ETA: 8:38"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x_pred_raw, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(preds)\n",
    "# Set column names to those generated by the one-hot encoding earlier\n",
    "col_names = one_hot.columns.values\n",
    "sub.columns = col_names\n",
    "# Insert the column id from the sample_submission at the start of the data frame\n",
    "sub.insert(0, 'id', df_test['id'])\n",
    "sub.head(5)\n",
    "# Saving results\n",
    "sub.to_csv(\"{}/results_{}.csv\".format(WORKING_DIR, trained_model), mode='w', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data to https://www.kaggle.com/c/dog-breed-identification/submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
